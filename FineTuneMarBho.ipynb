{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushm262003/FinalProj/blob/main/FineTuneMarBho.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5TBFqYh1frA",
        "outputId": "82950f32-9066-42cb-d8a4-8084d0bbbf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzXtsXPCgimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHMceyyZgkkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "XaUMFBHN2_I-",
        "outputId": "84e7b64f-af4b-4362-9402-8d4a3c2f3fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-86ec5238-48c8-4fce-b04a-fba0587933f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-86ec5238-48c8-4fce-b04a-fba0587933f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving probeless_ranking.pkl to probeless_ranking.pkl\n",
            "Saving mr_ufal-um-train.conllu to mr_ufal-um-train.conllu\n",
            "Saving linear by ttb probeless to linear by ttb probeless\n",
            "Saving linear by testing 1001-2000ttb probeless to linear by testing 1001-2000ttb probeless\n",
            "Saving bho_bhtb-um-test.conllu to bho_bhtb-um-test.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5zLNO9b3KW9",
        "outputId": "e7113695-5b01-46de-bdd5-ae222ccd4744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'mr_ufal-um-train.conllu', 'linear by testing 1001-2000ttb probeless', 'linear by ttb probeless', 'probeless_ranking.pkl', 'bho_bhtb-um-test.conllu', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from conllu import parse\n",
        "import numpy as np\n",
        "\n",
        "# Function to read .conllu files and extract (word, pos) pairs\n",
        "def read_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    parsed_data = parse(data)\n",
        "\n",
        "    sentences = []\n",
        "    for sentence in parsed_data:\n",
        "        word_tag_pairs = [(token[\"form\"], token[\"upos\"]) for token in sentence if token[\"form\"] is not None]\n",
        "        if word_tag_pairs:  # Only add non-empty sentences\n",
        "            sentences.append(word_tag_pairs)\n",
        "    return sentences\n",
        "\n",
        "# Load Marathi training data and Bhojpuri test data\n",
        "marathi_train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "bhojpuri_test_data = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Extract sentences and POS tags\n",
        "marathi_sentences, marathi_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in marathi_train_data])\n",
        "bhojpuri_sentences, bhojpuri_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in bhojpuri_test_data])\n",
        "\n",
        "# Create mapping of POS tags to IDs\n",
        "unique_tags = list(set(tag for sent in marathi_labels for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_tags))\n",
        "\n",
        "# Function to encode tags with proper subword handling\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    \"\"\"\n",
        "    Assign label to the first token of each word, and use -100 for subword tokens or special tokens.\n",
        "    \"\"\"\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id[tag] if tag in tag2id else -100)\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Define Dataset class\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, tag2id):\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        self.labels = []\n",
        "        for i, label in enumerate(labels):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            self.labels.append(encode_tags(label, word_ids, tag2id))\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = POSDataset(marathi_sentences, marathi_labels, tokenizer, tag2id)\n",
        "test_dataset = POSDataset(bhojpuri_sentences, bhojpuri_labels, tokenizer, tag2id)\n",
        "\n",
        "# Define training parameters (WandB logging disabled via report_to=[])\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=[]  # Disable WandB logging\n",
        ")\n",
        "\n",
        "# Define Trainer with evaluation dataset provided\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on Bhojpuri test data\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "    # Iterate sentence by sentence in the batch\n",
        "    for pred_sentence, true_sentence in zip(predictions, batch[\"labels\"].tolist()):\n",
        "        # Iterate token by token in the sentence\n",
        "        for pred_token, true_token in zip(pred_sentence, true_sentence):\n",
        "            if true_token != -100:\n",
        "                predicted_labels.append(id2tag[pred_token])\n",
        "                true_labels.append(id2tag[true_token])\n",
        "\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "xaDX5_T38iOA",
        "outputId": "174e2987-a8a5-4a04-8340-b0199da84da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='76' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 76/141 12:12 < 10:43, 0.10 it/s, Epoch 1.60/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.390600</td>\n",
              "      <td>2.438685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [141/141 24:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.390600</td>\n",
              "      <td>2.438685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.360500</td>\n",
              "      <td>2.008405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.807000</td>\n",
              "      <td>1.816220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        47\n",
            "         ADP       0.85      0.39      0.53       147\n",
            "         ADV       0.04      0.67      0.08         3\n",
            "         AUX       0.00      0.00      0.00        34\n",
            "       CCONJ       0.00      0.00      0.00        21\n",
            "         DET       0.33      0.02      0.04        47\n",
            "        NOUN       0.46      0.85      0.60       286\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.27      0.11      0.15        37\n",
            "       PROPN       0.00      0.00      0.00       110\n",
            "       PUNCT       0.88      0.93      0.91       121\n",
            "       SCONJ       0.00      0.00      0.00        16\n",
            "        VERB       0.39      0.81      0.53        95\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.49      1013\n",
            "   macro avg       0.22      0.25      0.19      1013\n",
            "weighted avg       0.42      0.49      0.41      1013\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(\"./saved_model\")\n",
        "tokenizer.save_pretrained(\"./saved_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiq0IOjmKiQA",
        "outputId": "dd35cb5f-c770-478e-b41a-44899146abb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./saved_model/tokenizer_config.json',\n",
              " './saved_model/special_tokens_map.json',\n",
              " './saved_model/vocab.txt',\n",
              " './saved_model/added_tokens.json',\n",
              " './saved_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, Trainer, TrainingArguments\n",
        "from pathlib import Path\n",
        "from load_subset import load_top_neurons\n",
        "import consts\n",
        "from dataHandler import UMDataHandler\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "# Custom BERT Model Using Selected Neurons\n",
        "class SubsetBERT(nn.Module):\n",
        "    def _init_(self, bert_model, selected_neurons, num_labels):\n",
        "        super(SubsetBERT, self)._init_()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model)\n",
        "        self.selected_neurons = selected_neurons  # Dictionary {layer: neuron_indices}\n",
        "        self.classifier = nn.Linear(sum(len(v) for v in selected_neurons.values()), num_labels)\n",
        "\n",
        "    def forward(self, input_embeds=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # Ensure input_embeds has correct shape [batch_size, seq_length, hidden_dim]\n",
        "        if input_embeds is not None and input_embeds.dim() == 1:\n",
        "            input_embeds = input_embeds.unsqueeze(0).unsqueeze(0)  # Convert [768] → [1, 1, 768]\n",
        "\n",
        "        outputs = self.bert(inputs_embeds=input_embeds, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Select specific neurons while keeping sequence length\n",
        "        selected_features = []\n",
        "        for layer, indices in self.selected_neurons.items():\n",
        "            selected_features.append(hidden_states[layer][:, :, indices])\n",
        "\n",
        "        selected_features = torch.cat(selected_features, dim=-1)  # Concatenate along last dim\n",
        "        selected_features = selected_features.mean(dim=1)  # Reduce sequence dimension if needed\n",
        "        logits = self.classifier(selected_features)\n",
        "\n",
        "        if labels is not None:\n",
        "            labels = labels.view(-1)  # Ensure labels have shape [batch_size]\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return loss, logits  # Ensure Trainer API gets loss\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Fine-tuning function with Trainer API\n",
        "def fine_tune_with_trainer(train_path, model_type, language, attribute, layer, subset_size=1000, epochs=1, save_path=\"\"):\n",
        "    print(\"\\n🔹 Loading Dataset...\")\n",
        "    data_handler = UMDataHandler(train_path, 'UM', model_type, layer, language=language, attribute=attribute)\n",
        "    data_handler.create_dicts()\n",
        "    dataloader = data_handler.get_dataloader(batch_size=32)\n",
        "    dataset = data_handler.get_dataset()\n",
        "    print(f\"✅ Dataset Loaded: {len(dataset)} samples\")\n",
        "\n",
        "    print(\"\\n🔹 Loading Selected Neurons...\")\n",
        "    model_name = consts.model_names[model_type]\n",
        "    num_labels = len(set([sample[1] for sample in dataloader.dataset]))\n",
        "    selected_neurons = load_top_neurons(Path('pickles', 'UM', model_type, language, attribute, 'probeless_ranking.pkl'), subset_size)\n",
        "    print(f\"✅ Selected {len(selected_neurons)} layers from {model_name}\")\n",
        "\n",
        "    print(\"\\n🔹 Initializing Model...\")\n",
        "    model = SubsetBERT(model_name, selected_neurons, num_labels)\n",
        "\n",
        "    # Print batch shapes for debugging\n",
        "    for batch in dataset:\n",
        "        for key, value in batch.items():\n",
        "            print(f\"{key}: {value.shape}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\n🔹 Setting Training Arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=str(save_path),\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=epochs,\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        save_total_limit=1,\n",
        "    )\n",
        "\n",
        "    print(\"\\n🔹 Initializing Trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    print(\"\\n🚀 Starting Fine-Tuning...\")\n",
        "    trainer.train()\n",
        "    print(\"✅ Fine-Tuning Complete!\")\n",
        "\n",
        "    if save_path:\n",
        "        model_filename = save_path / \"best_model.pth\"\n",
        "        torch.save(model.state_dict(), model_filename)\n",
        "        print(f\"📌 Best model saved to {model_filename}\")\n",
        "    else:\n",
        "        print(\"📌 No save path provided\")\n",
        "\n",
        "    print(\"\\n🏁 Training Finished!\")\n",
        "    return model\n",
        "\n",
        "# Main function\n",
        "if _name_ == \"_main_\":\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('-model', type=str, required=True)\n",
        "    parser.add_argument('-language', type=str, required=True)\n",
        "    parser.add_argument('-attribute', type=str, required=True)\n",
        "    parser.add_argument('-layer', type=int, required=True)\n",
        "    parser.add_argument('--control', default=False, action='store_true')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"\\n🔹 Parsing Arguments...\")\n",
        "    model_type = args.model\n",
        "    language = args.language\n",
        "    attribute = args.attribute\n",
        "    layer = args.layer\n",
        "    control = args.control\n",
        "    control_str = '_control' if control else ''\n",
        "\n",
        "    save_path = Path('pickles', 'UM', model_type, language, attribute)\n",
        "    train_path = Path('pickles', 'UM', model_type, language, 'train_parsed.pkl')\n",
        "\n",
        "    if not save_path.exists():\n",
        "        save_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"✅ Save Path: {save_path}\")\n",
        "    print(f\"✅ Training Data Path: {train_path}\")\n",
        "\n",
        "    # Fine-tune with Trainer API\n",
        "    fine_tune_with_trainer(train_path, model_type, language, attribute, layer, epochs=1, save_path=save_path)\n"
      ],
      "metadata": {
        "id": "CRHx-Fi6K0dA",
        "outputId": "c7d73f2a-1358-40a1-edf7-1f7a35f81a41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t   special_tokens_map.json  tokenizer.json\n",
            "model.safetensors  tokenizer_config.json    vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install transformers conllu scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
        "import pickle\n",
        "from conllu import parse_incr  # For parsing CoNLL-U files\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "##############################################\n",
        "# 1. Load the top 50 neuron indices from probeless_ranking.pkl\n",
        "##############################################\n",
        "pkl_path = \"/content/probeless_ranking.pkl\"  # Ensure this file is in your working directory\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "    ranking = pickle.load(f)\n",
        "\n",
        "# Select the top 50 indices (starting from the first value)\n",
        "top_50_indices = ranking[:50]\n",
        "print(\"Top 50 global neuron indices:\", top_50_indices)\n",
        "\n",
        "# Convert global indices to a dictionary mapping layer -> list of neuron indices.\n",
        "# Total neurons: 9984 = 13 layers * 768. In Hugging Face, hidden_states has 13 elements\n",
        "# (0: embeddings, 1-12: transformer layers).\n",
        "hidden_dim = 768\n",
        "selected_neurons = {}\n",
        "for idx in top_50_indices:\n",
        "    # Map the global index to a layer.\n",
        "    # Here layer 0 corresponds to the embedding output.\n",
        "    layer = idx // hidden_dim\n",
        "    neuron_idx = idx % hidden_dim\n",
        "    if layer not in selected_neurons:\n",
        "        selected_neurons[layer] = []\n",
        "    selected_neurons[layer].append(neuron_idx)\n",
        "\n",
        "print(\"Selected neurons by layer:\", selected_neurons)\n",
        "\n",
        "##############################################\n",
        "# 2. Define functions to read CoNLL-U files and build the POS tag datasets\n",
        "##############################################\n",
        "def read_conllu(file_path):\n",
        "    \"\"\"Reads a CoNLL-U file and returns a list of (words, pos_tags) for each sentence.\"\"\"\n",
        "    sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as data_file:\n",
        "        for tokenlist in parse_incr(data_file):\n",
        "            words = []\n",
        "            pos_tags = []\n",
        "            for token in tokenlist:\n",
        "                # Only consider tokens with a non-None form.\n",
        "                if token[\"form\"] is not None:\n",
        "                    words.append(token[\"form\"])\n",
        "                    # Use UPOS tag from the token; if missing, you can assign a default label.\n",
        "                    pos_tags.append(token.get(\"upos\", \"X\"))\n",
        "            if words:\n",
        "                sentences.append((words, pos_tags))\n",
        "    return sentences\n",
        "\n",
        "# Read training and test data (assumes POS tags are in the \"upos\" field)\n",
        "train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "test_data  = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Separate words and tags\n",
        "train_sentences, train_tags = zip(*train_data)\n",
        "test_sentences, test_tags   = zip(*test_data)\n",
        "\n",
        "# Build tag mappings from training data\n",
        "unique_tags = list(set(tag for sent in train_tags for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "print(\"Unique tags:\", unique_tags)\n",
        "print(\"Number of labels:\", len(unique_tags))\n",
        "\n",
        "##############################################\n",
        "# 3. Define a function to encode tags for token classification\n",
        "##############################################\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    \"\"\"\n",
        "    Assign label to the first token of each word, and -100 to subword or special tokens.\n",
        "    \"\"\"\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id[tag])\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "##############################################\n",
        "# 4. Define a Dataset class for token classification using CoNLL-U data\n",
        "##############################################\n",
        "class TokenClassificationDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, tokenizer, tag2id, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True,\n",
        "                                   padding=True, truncation=True, max_length=max_length,\n",
        "                                   return_tensors=\"pt\")\n",
        "        all_labels = []\n",
        "        for i, tag_seq in enumerate(tags):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            encoded_tags = encode_tags(tag_seq, word_ids, tag2id)\n",
        "            all_labels.append(encoded_tags)\n",
        "        # Pad labels manually if needed (tokenizer already padded input_ids)\n",
        "        self.labels = all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "##############################################\n",
        "# 5. Define the custom SubsetBERT model for token classification\n",
        "##############################################\n",
        "class SubsetBERT(nn.Module):\n",
        "    def __init__(self, bert_model_name, selected_neurons, num_labels):\n",
        "        super(SubsetBERT, self).__init__()\n",
        "        # Load the pre-trained BERT model and request hidden states.\n",
        "        # hidden_states will be a tuple of length 13 (0: embeddings, 1-12: transformer layers)\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
        "        self.selected_neurons = selected_neurons  # Dictionary: {layer: [indices]}\n",
        "        # The input dimension for each token is the sum of selected neurons across layers.\n",
        "        input_dim = sum(len(indices) for indices in selected_neurons.values())\n",
        "        # For token classification, classifier outputs [batch, seq_len, num_labels]\n",
        "        self.classifier = nn.Linear(input_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        selected_features = []\n",
        "        for layer, indices in self.selected_neurons.items():\n",
        "            # hidden_states[layer] has shape: [batch_size, seq_length, hidden_dim]\n",
        "            layer_features = hidden_states[layer][:, :, indices]\n",
        "            selected_features.append(layer_features)\n",
        "        # Concatenate along the hidden dimension: shape [batch, seq_len, total_selected_neurons]\n",
        "        selected_features = torch.cat(selected_features, dim=-1)\n",
        "        logits = self.classifier(selected_features)  # [batch, seq_len, num_labels]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            # Flatten the tokens\n",
        "            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "##############################################\n",
        "# 6. Prepare the tokenizer, datasets, and model\n",
        "##############################################\n",
        "bert_model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "train_dataset = TokenClassificationDataset(train_sentences, train_tags, tokenizer, tag2id, max_length=128)\n",
        "test_dataset  = TokenClassificationDataset(test_sentences, test_tags, tokenizer, tag2id, max_length=128)\n",
        "\n",
        "num_labels = len(unique_tags)\n",
        "print(\"Number of labels for token classification:\", num_labels)\n",
        "\n",
        "# Initialize the custom model for token classification\n",
        "model = SubsetBERT(bert_model_name, selected_neurons, num_labels)\n",
        "print(\"Custom SubsetBERT model (for token classification) initialized.\")\n",
        "\n",
        "##############################################\n",
        "# 7. Set up the Trainer API and start training\n",
        "##############################################\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[],  # Disable WandB logging\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting training ...\")\n",
        "trainer.train()\n",
        "print(\"Training completed.\")\n",
        "\n",
        "##############################################\n",
        "# 8. Save the fine-tuned model\n",
        "##############################################\n",
        "model_save_path = Path(\"./fine_tuned_subsetbert\")\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), model_save_path / \"best_model.pth\")\n",
        "print(f\"Model saved to {model_save_path / 'best_model.pth'}\")\n",
        "\n",
        "##############################################\n",
        "# 9. Evaluate on the test set and print classification report\n",
        "##############################################\n",
        "# Run predictions on the test set\n",
        "predictions_output = trainer.predict(test_dataset)\n",
        "# predictions_output.predictions shape: [num_samples, seq_len, num_labels]\n",
        "pred_logits = predictions_output.predictions\n",
        "true_labels = predictions_output.label_ids\n",
        "\n",
        "# Convert logits to predicted labels (for each token)\n",
        "pred_labels = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "# Gather predictions and true labels (ignoring label -100)\n",
        "all_pred_tags = []\n",
        "all_true_tags = []\n",
        "for i in range(len(true_labels)):\n",
        "    for j in range(len(true_labels[i])):\n",
        "        if true_labels[i][j] != -100:\n",
        "            all_true_tags.append(id2tag[true_labels[i][j]])\n",
        "            all_pred_tags.append(id2tag[pred_labels[i][j]])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(all_true_tags, all_pred_tags, zero_division=0)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5OYAw6wHqSJb",
        "outputId": "6aab0c97-ce65-479c-ebef-80a86991f431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Top 50 global neuron indices: [8675, 7907, 6371, 7139, 5603, 4821, 8426, 9194, 3108, 5589, 2499, 7658, 5867, 4860, 6890, 4092, 5628, 8356, 3324, 7588, 3961, 6052, 6820, 1471, 3285, 3337, 3734, 1457, 6635, 2326, 9124, 804, 1166, 398, 5412, 5171, 2569, 7809, 2340, 3193, 3215, 6396, 2556, 4729, 3748, 1572, 1558, 5186, 6875, 3173]\n",
            "Selected neurons by layer: {11: [227, 746, 676], 10: [227, 746, 676, 129], 8: [227, 746, 676, 491, 252, 731], 9: [227, 746, 676], 7: [227, 213, 491, 252, 676, 36], 6: [213, 252, 563, 121, 578], 4: [36, 252, 213, 265, 662, 121, 143, 676, 101], 3: [195, 22, 265, 36, 252], 5: [252, 121], 1: [703, 689, 36, 398], 0: [398], 2: [36, 22]}\n",
            "Unique tags: ['PUNCT', 'INTJ', 'DET', 'CCONJ', 'SCONJ', 'AUX', 'X', 'PRON', 'ADV', 'NOUN', 'VERB', 'PART', 'NUM', 'PROPN', 'ADJ', 'ADP', '_']\n",
            "Number of labels: 17\n",
            "Number of labels for token classification: 17\n",
            "Custom SubsetBERT model (for token classification) initialized.\n",
            "Starting training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [235/235 48:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.859638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.442700</td>\n",
              "      <td>1.983034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.515400</td>\n",
              "      <td>2.140307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.275400</td>\n",
              "      <td>2.057409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.180600</td>\n",
              "      <td>2.134906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n",
            "Model saved to fine_tuned_subsetbert/best_model.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        45\n",
            "         ADP       0.71      0.32      0.44       146\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.00      0.00      0.00        34\n",
            "       CCONJ       0.00      0.00      0.00        21\n",
            "         DET       0.09      0.02      0.03        47\n",
            "        NOUN       0.50      0.82      0.62       284\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.22      0.11      0.15        37\n",
            "       PROPN       0.06      0.01      0.02        99\n",
            "       PUNCT       0.88      0.85      0.87       115\n",
            "       SCONJ       0.00      0.00      0.00        16\n",
            "        VERB       0.47      0.64      0.54        94\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.45       990\n",
            "   macro avg       0.20      0.19      0.18       990\n",
            "weighted avg       0.41      0.45      0.40       990\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select your probeless_ranking.pkl file when prompted.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "xYjvIqupqvfV",
        "outputId": "8af2dde0-dae1-48f2-cdf6-3d8322d7d3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-42e17958-c385-4986-9ffe-406043d5248a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-42e17958-c385-4986-9ffe-406043d5248a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50ad62161823>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Select your probeless_ranking.pkl file when prompted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from conllu import parse\n",
        "import numpy as np\n",
        "\n",
        "# Function to read .conllu files and extract (word, pos) pairs\n",
        "def read_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    parsed_data = parse(data)\n",
        "\n",
        "    sentences = []\n",
        "    for sentence in parsed_data:\n",
        "        word_tag_pairs = [(token[\"form\"], token[\"upos\"]) for token in sentence if token[\"form\"] is not None]\n",
        "        if word_tag_pairs:  # Only add non-empty sentences\n",
        "            sentences.append(word_tag_pairs)\n",
        "    return sentences\n",
        "\n",
        "# Load Marathi training data and Bhojpuri test data\n",
        "marathi_train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "bhojpuri_test_data = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Extract sentences and POS tags\n",
        "marathi_sentences, marathi_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in marathi_train_data])\n",
        "bhojpuri_sentences, bhojpuri_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in bhojpuri_test_data])\n",
        "\n",
        "# Create mapping of POS tags to IDs\n",
        "unique_tags = list(set(tag for sent in marathi_labels for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_tags))\n",
        "\n",
        "# 🔄 Freeze all mBERT parameters (only classifier head will be trained)\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Function to encode tags with proper subword handling\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id[tag] if tag in tag2id else -100)\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Define Dataset class\n",
        "class POSDataset(Dataset):\n",
        "    def _init_(self, sentences, labels, tokenizer, tag2id):\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        self.labels = []\n",
        "        for i, label in enumerate(labels):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            self.labels.append(encode_tags(label, word_ids, tag2id))\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def _len_(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def _getitem_(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = POSDataset(marathi_sentences, marathi_labels, tokenizer, tag2id)\n",
        "test_dataset = POSDataset(bhojpuri_sentences, bhojpuri_labels, tokenizer, tag2id)\n",
        "\n",
        "# Define training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.05,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=[]  # Disable WandB logging\n",
        ")\n",
        "\n",
        "# Define Trainer with evaluation dataset provided\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model (only classifier head will be trained)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on Bhojpuri test data\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "    for pred_sentence, true_sentence in zip(predictions, batch[\"labels\"].tolist()):\n",
        "        for pred_token, true_token in zip(pred_sentence, true_sentence):\n",
        "            if true_token != -100:\n",
        "                predicted_labels.append(id2tag[pred_token])\n",
        "                true_labels.append(id2tag[true_token])\n",
        "\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "qcoRHuDag1Hi",
        "outputId": "3774e0d7-5691-4310-a305-dd17a7fbc4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'conllu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a482fd301e1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconllu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'conllu'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from conllu import parse\n",
        "import numpy as np\n",
        "\n",
        "# Function to read .conllu files and extract (word, pos) pairs\n",
        "def read_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    parsed_data = parse(data)\n",
        "\n",
        "    sentences = []\n",
        "    for sentence in parsed_data:\n",
        "        word_tag_pairs = [(token[\"form\"], token[\"upos\"]) for token in sentence if token[\"form\"] is not None]\n",
        "        if word_tag_pairs:  # Only add non-empty sentences\n",
        "            sentences.append(word_tag_pairs)\n",
        "    return sentences\n",
        "\n",
        "# Load Marathi training data and Bhojpuri test data\n",
        "marathi_train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "bhojpuri_test_data = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Extract sentences and POS tags\n",
        "marathi_sentences, marathi_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in marathi_train_data])\n",
        "bhojpuri_sentences, bhojpuri_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in bhojpuri_test_data])\n",
        "\n",
        "# Create mapping of POS tags to IDs\n",
        "unique_tags = list(set(tag for sent in marathi_labels for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_tags))\n",
        "\n",
        "# 🔄 Freeze all mBERT parameters (only classifier head will be trained)\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Function to encode tags with proper subword handling\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id.get(tag, -100))\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Define Dataset class\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, tag2id):\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        self.labels = []\n",
        "        for i, label in enumerate(labels):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            self.labels.append(encode_tags(label, word_ids, tag2id))\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = POSDataset(marathi_sentences, marathi_labels, tokenizer, tag2id)\n",
        "test_dataset = POSDataset(bhojpuri_sentences, bhojpuri_labels, tokenizer, tag2id)\n",
        "\n",
        "# Define training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.05,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=[]  # Disable WandB logging\n",
        ")\n",
        "\n",
        "# Define Trainer with evaluation dataset provided\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model (only classifier head will be trained)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on Bhojpuri test data\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "    for pred_sentence, true_sentence in zip(predictions, batch[\"labels\"].tolist()):\n",
        "        for pred_token, true_token in zip(pred_sentence, true_sentence):\n",
        "            if true_token != -100:\n",
        "                predicted_labels.append(id2tag[pred_token])\n",
        "                true_labels.append(id2tag[true_token])\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dvThtDNdT8Ip",
        "outputId": "d7f1375c-23fb-49c2-8098-3421b88dddec"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='109' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [109/120 47:29 < 04:52, 0.04 it/s, Epoch 18/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.854208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.889200</td>\n",
              "      <td>2.853760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.889200</td>\n",
              "      <td>2.852998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.889500</td>\n",
              "      <td>2.851912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.889800</td>\n",
              "      <td>2.850527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.889800</td>\n",
              "      <td>2.848835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.879100</td>\n",
              "      <td>2.846819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.879100</td>\n",
              "      <td>2.844498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.873700</td>\n",
              "      <td>2.841886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.869900</td>\n",
              "      <td>2.838978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.869900</td>\n",
              "      <td>2.835769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.859700</td>\n",
              "      <td>2.832290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.859700</td>\n",
              "      <td>2.828510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.839600</td>\n",
              "      <td>2.824437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.831600</td>\n",
              "      <td>2.820123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.831600</td>\n",
              "      <td>2.815541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.819900</td>\n",
              "      <td>2.810675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 53:31, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.854208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.889200</td>\n",
              "      <td>2.853760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.889200</td>\n",
              "      <td>2.852998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.889500</td>\n",
              "      <td>2.851912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.889800</td>\n",
              "      <td>2.850527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.889800</td>\n",
              "      <td>2.848835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.879100</td>\n",
              "      <td>2.846819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.879100</td>\n",
              "      <td>2.844498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.873700</td>\n",
              "      <td>2.841886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.869900</td>\n",
              "      <td>2.838978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.869900</td>\n",
              "      <td>2.835769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.859700</td>\n",
              "      <td>2.832290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.859700</td>\n",
              "      <td>2.828510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.839600</td>\n",
              "      <td>2.824437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.831600</td>\n",
              "      <td>2.820123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.831600</td>\n",
              "      <td>2.815541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.819900</td>\n",
              "      <td>2.810675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.819900</td>\n",
              "      <td>2.805507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.801500</td>\n",
              "      <td>2.800076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.786900</td>\n",
              "      <td>2.794448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        47\n",
            "         ADP       0.20      0.19      0.20       147\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.04      0.03      0.04        34\n",
            "       CCONJ       0.06      0.33      0.10        21\n",
            "         DET       0.00      0.00      0.00        47\n",
            "        INTJ       0.00      0.00      0.00         0\n",
            "        NOUN       0.29      0.03      0.06       286\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.05      0.19      0.08        37\n",
            "       PROPN       0.21      0.15      0.18       110\n",
            "       PUNCT       0.21      0.05      0.08       121\n",
            "       SCONJ       0.01      0.06      0.02        16\n",
            "        VERB       0.07      0.09      0.08        95\n",
            "           X       0.00      0.00      0.00         0\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.08      1013\n",
            "   macro avg       0.07      0.07      0.05      1013\n",
            "weighted avg       0.17      0.08      0.09      1013\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter, defaultdict\n",
        "from conllu import parse\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Function to read .conllu files and extract (word, pos) pairs\n",
        "def read_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    parsed_data = parse(data)\n",
        "\n",
        "    sentences = []\n",
        "    for sentence in parsed_data:\n",
        "        word_tag_pairs = [(token[\"form\"], token[\"upos\"]) for token in sentence if token[\"form\"] is not None]\n",
        "        if word_tag_pairs:\n",
        "            sentences.append(word_tag_pairs)\n",
        "    return sentences\n",
        "\n",
        "# Load training (Marathi) and test (Bhojpuri) data\n",
        "marathi_train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "bhojpuri_test_data = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Extract sentences and POS tags\n",
        "marathi_sentences, marathi_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in marathi_train_data])\n",
        "bhojpuri_sentences, bhojpuri_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in bhojpuri_test_data])\n",
        "\n",
        "# Create mapping of POS tags to IDs\n",
        "unique_tags = list(set(tag for sent in marathi_labels for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# 🔥 Apply Oversampling (Word-Level) to Balance Marathi Training Data\n",
        "word_tag_pairs = [(word, tag) for sent, tags in zip(marathi_sentences, marathi_labels) for word, tag in zip(sent, tags)]\n",
        "words, tags = zip(*word_tag_pairs)\n",
        "words = np.array(words).reshape(-1, 1)\n",
        "tags = np.array(tags)\n",
        "\n",
        "sampling_strategy = {tag: max(50, count) for tag, count in Counter(tags).items()}\n",
        "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
        "words_resampled, tags_resampled = ros.fit_resample(words, tags)\n",
        "\n",
        "# Reconstruct sentences after oversampling\n",
        "tagged_sentences = defaultdict(list)\n",
        "for word, tag in zip(words_resampled.flatten(), tags_resampled):\n",
        "    tagged_sentences[tag].append(word)\n",
        "\n",
        "marathi_sentences_resampled = []\n",
        "marathi_labels_resampled = []\n",
        "for tag, word_list in tagged_sentences.items():\n",
        "    marathi_sentences_resampled.append(word_list)\n",
        "    marathi_labels_resampled.append([tag] * len(word_list))\n",
        "\n",
        "# Function to encode tags with subword tokenization handling\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id.get(tag, -100))\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Define POSDataset class\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, tag2id):\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        self.labels = []\n",
        "        for i, label in enumerate(labels):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            self.labels.append(encode_tags(label, word_ids, tag2id))\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Prepare training and testing datasets\n",
        "train_dataset = POSDataset(marathi_sentences_resampled, marathi_labels_resampled, tokenizer, tag2id)\n",
        "test_dataset = POSDataset(bhojpuri_sentences, bhojpuri_labels, tokenizer, tag2id)\n",
        "\n",
        "# Load BERT model with a classification head\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_tags))\n",
        "\n",
        "# 🔄 Freeze all BERT parameters except classifier head\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 🔓 Unfreeze the last 6 layers for fine-tuning\n",
        "for layer in model.bert.encoder.layer[-6:]:  # Last 6 layers\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Training arguments with learning rate scheduling\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=50,  # Increased for better generalization\n",
        "    per_device_train_batch_size=8,  # Increased batch size\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.05,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,  # Adjusted for stability\n",
        "    report_to=[]  # Disable WandB logging\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate model on Bhojpuri test set\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "    for pred_sentence, true_sentence in zip(predictions, batch[\"labels\"].tolist()):\n",
        "        for pred_token, true_token in zip(pred_sentence, true_sentence):\n",
        "            if true_token != -100:\n",
        "                predicted_labels.append(id2tag[pred_token])\n",
        "                true_labels.append(id2tag[true_token])\n",
        "\n",
        "# Print final classification report\n",
        "print(\"\\n🔎 **Evaluation Results on Bhojpuri Test Set:**\")\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8a_DE13nfQm",
        "outputId": "731b04a1-4302-4ab5-95fa-e2173d9156c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/150 1:27:47 < 17:50, 0.02 it/s, Epoch 41.33/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.782729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.782291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.781909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.781710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.781400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.780714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.779681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.778185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.776513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.774205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.772022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.769089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.764688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.760419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.756472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.753303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.749846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.746200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.741377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.737205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.733129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.726199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.720659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.716238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.711947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.708066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.705220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.701881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.699337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.696013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.695655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.692813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.686491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.679621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.674775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.668733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.662606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.658528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.657283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.651993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.649841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 1:45:38, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.782729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.782291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.781909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.781710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.781400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.880600</td>\n",
              "      <td>2.780714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.779681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.778185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.767600</td>\n",
              "      <td>2.776513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.774205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.772022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.769089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.705000</td>\n",
              "      <td>2.764688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.760419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.756472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.724900</td>\n",
              "      <td>2.753303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.749846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.746200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.692000</td>\n",
              "      <td>2.741377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.737205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.733129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.726199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.720659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.716238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.711947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.457500</td>\n",
              "      <td>2.708066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.705220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.701881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.699337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.696013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.695655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.692813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.352600</td>\n",
              "      <td>2.686491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.679621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.674775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.136800</td>\n",
              "      <td>2.668733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.662606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.658528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.752900</td>\n",
              "      <td>2.657283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.651993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.649841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.647656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.702400</td>\n",
              "      <td>2.640609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.544500</td>\n",
              "      <td>2.637947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.544500</td>\n",
              "      <td>2.633905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.544500</td>\n",
              "      <td>2.629811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.206900</td>\n",
              "      <td>2.634283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.206900</td>\n",
              "      <td>2.636367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.206900</td>\n",
              "      <td>2.635011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.069500</td>\n",
              "      <td>2.637819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 **Evaluation Results on Bhojpuri Test Set:**\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.25      0.04      0.07        47\n",
            "         ADP       0.09      0.01      0.01       147\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.43      0.09      0.15        34\n",
            "       CCONJ       0.00      0.00      0.00        21\n",
            "         DET       0.00      0.00      0.00        47\n",
            "        INTJ       0.00      0.00      0.00         0\n",
            "        NOUN       0.25      0.48      0.33       286\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.00      0.00      0.00        37\n",
            "       PROPN       0.00      0.00      0.00       110\n",
            "       PUNCT       0.50      0.01      0.02       121\n",
            "       SCONJ       0.00      0.00      0.00        16\n",
            "        VERB       0.23      0.26      0.24        95\n",
            "           X       0.00      0.00      0.00         0\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.17      1013\n",
            "   macro avg       0.10      0.05      0.05      1013\n",
            "weighted avg       0.19      0.17      0.13      1013\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hLMEpEcF73ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "TvBLvcCIUJv4",
        "outputId": "603d6fd4-8338-4505-a972-9737a4f24e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5e20fb4b-64ba-4cdb-9803-e49e5201b2db\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5e20fb4b-64ba-4cdb-9803-e49e5201b2db\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving probeless_ranking.pkl to probeless_ranking.pkl\n",
            "Saving mr_ufal-um-train.conllu to mr_ufal-um-train.conllu\n",
            "Saving linear by ttb probeless to linear by ttb probeless\n",
            "Saving linear by testing 1001-2000ttb probeless to linear by testing 1001-2000ttb probeless\n",
            "Saving bho_bhtb-um-test.conllu to bho_bhtb-um-test.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install transformers conllu scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
        "import pickle\n",
        "from conllu import parse_incr  # For parsing CoNLL-U files\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "##############################################\n",
        "# 1. Load the top 50 neuron indices from probeless_ranking.pkl\n",
        "##############################################\n",
        "pkl_path = \"/content/probeless_ranking.pkl\"  # Ensure this file is in your working directory\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "    ranking = pickle.load(f)\n",
        "\n",
        "# Select the top 50 indices (starting from the first value)\n",
        "top_50_indices = ranking[:50]\n",
        "\n",
        "# Convert global indices to a dictionary mapping layer -> list of neuron indices.\n",
        "hidden_dim = 768\n",
        "selected_neurons = {}\n",
        "for idx in top_50_indices:\n",
        "    layer = idx // hidden_dim\n",
        "    neuron_idx = idx % hidden_dim\n",
        "    if layer not in selected_neurons:\n",
        "        selected_neurons[layer] = []\n",
        "    selected_neurons[layer].append(neuron_idx)\n",
        "\n",
        "##############################################\n",
        "# 2. Define functions to read CoNLL-U files and build the POS tag datasets\n",
        "##############################################\n",
        "def read_conllu(file_path):\n",
        "    \"\"\"Reads a CoNLL-U file and returns a list of (words, pos_tags) for each sentence.\"\"\"\n",
        "    sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as data_file:\n",
        "        for tokenlist in parse_incr(data_file):\n",
        "            words = []\n",
        "            pos_tags = []\n",
        "            for token in tokenlist:\n",
        "                if token[\"form\"] is not None:\n",
        "                    words.append(token[\"form\"])\n",
        "                    pos_tags.append(token.get(\"upos\", \"X\"))\n",
        "            if words:\n",
        "                sentences.append((words, pos_tags))\n",
        "    return sentences\n",
        "\n",
        "# Read training and test data\n",
        "train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "test_data  = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Separate words and tags\n",
        "train_sentences, train_tags = zip(*train_data)\n",
        "test_sentences, test_tags   = zip(*test_data)\n",
        "\n",
        "# Build tag mappings from training data\n",
        "unique_tags = list(set(tag for sent in train_tags for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "##############################################\n",
        "# 3. Define a function to encode tags for token classification\n",
        "##############################################\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    \"\"\"Assign label to the first token of each word, and -100 to subword or special tokens.\"\"\"\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id[tag])\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "##############################################\n",
        "# 4. Define a Dataset class for token classification\n",
        "##############################################\n",
        "class TokenClassificationDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, tokenizer, tag2id, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True,\n",
        "                                   padding=True, truncation=True, max_length=max_length,\n",
        "                                   return_tensors=\"pt\")\n",
        "        all_labels = []\n",
        "        for i, tag_seq in enumerate(tags):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            encoded_tags = encode_tags(tag_seq, word_ids, tag2id)\n",
        "            all_labels.append(encoded_tags)\n",
        "        self.labels = all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "##############################################\n",
        "# 5. Define the custom SubsetBERT model for token classification\n",
        "##############################################\n",
        "class SubsetBERT(nn.Module):\n",
        "    def __init__(self, bert_model_name, selected_neurons, num_labels):\n",
        "        super(SubsetBERT, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
        "        self.selected_neurons = selected_neurons\n",
        "\n",
        "        # Freeze all BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Compute input dimension from selected neurons\n",
        "        input_dim = sum(len(indices) for indices in selected_neurons.values())\n",
        "        self.classifier = nn.Linear(input_dim, num_labels)  # Trainable classifier\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        selected_features = []\n",
        "        for layer, indices in self.selected_neurons.items():\n",
        "            layer_features = hidden_states[layer][:, :, indices]\n",
        "            selected_features.append(layer_features)\n",
        "\n",
        "        selected_features = torch.cat(selected_features, dim=-1)\n",
        "        logits = self.classifier(selected_features)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "##############################################\n",
        "# 6. Prepare the tokenizer, datasets, and model\n",
        "##############################################\n",
        "bert_model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "train_dataset = TokenClassificationDataset(train_sentences, train_tags, tokenizer, tag2id, max_length=128)\n",
        "test_dataset  = TokenClassificationDataset(test_sentences, test_tags, tokenizer, tag2id, max_length=128)\n",
        "\n",
        "num_labels = len(unique_tags)\n",
        "\n",
        "# Initialize the custom model\n",
        "model = SubsetBERT(bert_model_name, selected_neurons, num_labels)\n",
        "\n",
        "##############################################\n",
        "# 7. Set up the Trainer API and start training\n",
        "##############################################\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "##############################################\n",
        "# 8. Save the fine-tuned model\n",
        "##############################################\n",
        "model_save_path = Path(\"./fine_tuned_subsetbert\")\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), model_save_path / \"best_model.pth\")\n",
        "\n",
        "##############################################\n",
        "# 9. Evaluate on the test set\n",
        "##############################################\n",
        "predictions_output = trainer.predict(test_dataset)\n",
        "pred_logits = predictions_output.predictions\n",
        "true_labels = predictions_output.label_ids\n",
        "\n",
        "pred_labels = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "# Gather predictions and true labels (ignoring label -100)\n",
        "all_pred_tags = []\n",
        "all_true_tags = []\n",
        "for i in range(len(true_labels)):\n",
        "    for j in range(len(true_labels[i])):\n",
        "        if true_labels[i][j] != -100:\n",
        "            all_true_tags.append(id2tag[true_labels[i][j]])\n",
        "            all_pred_tags.append(id2tag[pred_labels[i][j]])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(all_true_tags, all_pred_tags, zero_division=0)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-8ywtbuSpGfp",
        "outputId": "eb2b1058-f6a2-404e-ac06-b85761f30a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [235/235 12:22, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.603316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.518100</td>\n",
              "      <td>7.301453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.110700</td>\n",
              "      <td>7.089004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.828700</td>\n",
              "      <td>6.962329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.669300</td>\n",
              "      <td>6.920300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        45\n",
            "         ADP       0.00      0.00      0.00       146\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.00      0.00      0.00        34\n",
            "       CCONJ       0.00      0.00      0.00        21\n",
            "         DET       0.00      0.00      0.00        47\n",
            "        NOUN       0.00      0.00      0.00       284\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.02      1.00      0.03        16\n",
            "        PRON       0.00      0.00      0.00        37\n",
            "       PROPN       0.00      0.00      0.00        99\n",
            "       PUNCT       0.00      0.00      0.00       115\n",
            "       SCONJ       0.00      0.00      0.00        16\n",
            "        VERB       0.00      0.00      0.00        94\n",
            "\n",
            "    accuracy                           0.02       990\n",
            "   macro avg       0.00      0.07      0.00       990\n",
            "weighted avg       0.00      0.02      0.00       990\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
        "import pickle\n",
        "from conllu import parse_incr  # For parsing CoNLL-U files\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the top 50 neuron indices\n",
        "pkl_path = \"/content/probeless_ranking.pkl\"\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "    ranking = pickle.load(f)\n",
        "\n",
        "top_50_indices = ranking[:50]\n",
        "hidden_dim = 768\n",
        "selected_neurons = {}\n",
        "for idx in top_50_indices:\n",
        "    layer = idx // hidden_dim\n",
        "    neuron_idx = idx % hidden_dim\n",
        "    if layer not in selected_neurons:\n",
        "        selected_neurons[layer] = []\n",
        "    selected_neurons[layer].append(neuron_idx)\n",
        "\n",
        "# Function to read CoNLL-U files\n",
        "def read_conllu(file_path):\n",
        "    sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as data_file:\n",
        "        for tokenlist in parse_incr(data_file):\n",
        "            words = []\n",
        "            pos_tags = []\n",
        "            for token in tokenlist:\n",
        "                if token[\"form\"] is not None:\n",
        "                    words.append(token[\"form\"])\n",
        "                    pos_tags.append(token.get(\"upos\", \"X\"))\n",
        "            if words:\n",
        "                sentences.append((words, pos_tags))\n",
        "    return sentences\n",
        "\n",
        "# Read data\n",
        "train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "test_data  = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "train_sentences, train_tags = zip(*train_data)\n",
        "test_sentences, test_tags   = zip(*test_data)\n",
        "\n",
        "unique_tags = list(set(tag for sent in train_tags for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# Encode tags\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id[tag])\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Dataset class\n",
        "class TokenClassificationDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, tokenizer, tag2id, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True,\n",
        "                                   padding=True, truncation=True, max_length=max_length,\n",
        "                                   return_tensors=\"pt\")\n",
        "        all_labels = []\n",
        "        for i, tag_seq in enumerate(tags):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            encoded_tags = encode_tags(tag_seq, word_ids, tag2id)\n",
        "            all_labels.append(encoded_tags)\n",
        "        self.labels = all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Custom SubsetBERT model\n",
        "class SubsetBERT(nn.Module):\n",
        "    def __init__(self, bert_model_name, selected_neurons, num_labels):\n",
        "        super(SubsetBERT, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
        "        self.selected_neurons = selected_neurons\n",
        "\n",
        "        # Freeze all layers first\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze last 6 layers\n",
        "        for layer in range(-6, 0):\n",
        "            for param in self.bert.encoder.layer[layer].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # Compute input dimension from selected neurons\n",
        "        input_dim = sum(len(indices) for indices in selected_neurons.values())\n",
        "        self.classifier = nn.Linear(input_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        selected_features = []\n",
        "        for layer, indices in self.selected_neurons.items():\n",
        "            layer_features = hidden_states[layer][:, :, indices]\n",
        "            selected_features.append(layer_features)\n",
        "\n",
        "        selected_features = torch.cat(selected_features, dim=-1)\n",
        "        logits = self.classifier(selected_features)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "# Prepare tokenizer, datasets, and model\n",
        "bert_model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "train_dataset = TokenClassificationDataset(train_sentences, train_tags, tokenizer, tag2id, max_length=128)\n",
        "test_dataset  = TokenClassificationDataset(test_sentences, test_tags, tokenizer, tag2id, max_length=128)\n",
        "\n",
        "num_labels = len(unique_tags)\n",
        "model = SubsetBERT(bert_model_name, selected_neurons, num_labels)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model_save_path = Path(\"./fine_tuned_subsetbert\")\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), model_save_path / \"best_model.pth\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "predictions_output = trainer.predict(test_dataset)\n",
        "pred_logits = predictions_output.predictions\n",
        "true_labels = predictions_output.label_ids\n",
        "\n",
        "pred_labels = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "# Gather predictions and true labels\n",
        "all_pred_tags = []\n",
        "all_true_tags = []\n",
        "for i in range(len(true_labels)):\n",
        "    for j in range(len(true_labels[i])):\n",
        "        if true_labels[i][j] != -100:\n",
        "            all_true_tags.append(id2tag[true_labels[i][j]])\n",
        "            all_pred_tags.append(id2tag[pred_labels[i][j]])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(all_true_tags, all_pred_tags, zero_division=0)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "CHRswDG51hh2",
        "outputId": "3a4c324c-469b-4368-b7ce-2d336d9262bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [235/235 20:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.319722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.169900</td>\n",
              "      <td>2.079341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.971900</td>\n",
              "      <td>2.163758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.697800</td>\n",
              "      <td>2.086117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.547300</td>\n",
              "      <td>2.099670</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        45\n",
            "         ADP       0.71      0.35      0.47       146\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.29      0.06      0.10        34\n",
            "       CCONJ       0.00      0.00      0.00        21\n",
            "         DET       0.25      0.11      0.15        47\n",
            "        NOUN       0.60      0.66      0.63       284\n",
            "         NUM       1.00      0.03      0.06        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.33      0.11      0.16        37\n",
            "       PROPN       0.67      0.08      0.14        99\n",
            "       PUNCT       0.86      0.89      0.87       115\n",
            "       SCONJ       0.00      0.00      0.00        16\n",
            "        VERB       0.34      0.79      0.47        94\n",
            "           X       0.00      0.00      0.00         0\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.44       990\n",
            "   macro avg       0.31      0.19      0.19       990\n",
            "weighted avg       0.54      0.44      0.43       990\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter, defaultdict\n",
        "from conllu import parse\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Function to read .conllu files and extract (word, pos) pairs\n",
        "def read_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "    parsed_data = parse(data)\n",
        "\n",
        "    sentences = []\n",
        "    for sentence in parsed_data:\n",
        "        word_tag_pairs = [(token[\"form\"], token[\"upos\"]) for token in sentence if token[\"form\"] is not None]\n",
        "        if word_tag_pairs:\n",
        "            sentences.append(word_tag_pairs)\n",
        "    return sentences\n",
        "\n",
        "# Load training (Marathi) and test (Bhojpuri) data\n",
        "marathi_train_data = read_conllu(\"/content/mr_ufal-um-train.conllu\")\n",
        "bhojpuri_test_data = read_conllu(\"/content/bho_bhtb-um-test.conllu\")\n",
        "\n",
        "# Extract sentences and POS tags\n",
        "marathi_sentences, marathi_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in marathi_train_data])\n",
        "bhojpuri_sentences, bhojpuri_labels = zip(*[(list(zip(*sent))[0], list(zip(*sent))[1]) for sent in bhojpuri_test_data])\n",
        "\n",
        "# Create mapping of POS tags to IDs\n",
        "unique_tags = list(set(tag for sent in marathi_labels for tag in sent))\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# 🔥 Apply Oversampling (Word-Level) to Balance Marathi Training Data\n",
        "word_tag_pairs = [(word, tag) for sent, tags in zip(marathi_sentences, marathi_labels) for word, tag in zip(sent, tags)]\n",
        "words, tags = zip(*word_tag_pairs)\n",
        "words = np.array(words).reshape(-1, 1)\n",
        "tags = np.array(tags)\n",
        "\n",
        "sampling_strategy = {tag: max(50, count) for tag, count in Counter(tags).items()}\n",
        "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
        "words_resampled, tags_resampled = ros.fit_resample(words, tags)\n",
        "\n",
        "# Reconstruct sentences after oversampling\n",
        "tagged_sentences = defaultdict(list)\n",
        "for word, tag in zip(words_resampled.flatten(), tags_resampled):\n",
        "    tagged_sentences[tag].append(word)\n",
        "\n",
        "marathi_sentences_resampled = []\n",
        "marathi_labels_resampled = []\n",
        "for tag, word_list in tagged_sentences.items():\n",
        "    marathi_sentences_resampled.append(word_list)\n",
        "    marathi_labels_resampled.append([tag] * len(word_list))\n",
        "\n",
        "# Function to encode tags with subword tokenization handling\n",
        "def encode_tags(tags, word_ids, tag2id):\n",
        "    encoded_tags = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            encoded_tags.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            tag = tags[word_idx]\n",
        "            encoded_tags.append(tag2id.get(tag, -100))\n",
        "        else:\n",
        "            encoded_tags.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return encoded_tags\n",
        "\n",
        "# Define POSDataset class\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, tokenizer, tag2id):\n",
        "        self.encodings = tokenizer(list(sentences), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        self.labels = []\n",
        "        for i, label in enumerate(labels):\n",
        "            word_ids = self.encodings.word_ids(batch_index=i)\n",
        "            self.labels.append(encode_tags(label, word_ids, tag2id))\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.encodings[key][idx] for key in self.encodings.keys()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Prepare training and testing datasets\n",
        "train_dataset = POSDataset(marathi_sentences_resampled, marathi_labels_resampled, tokenizer, tag2id)\n",
        "test_dataset = POSDataset(bhojpuri_sentences, bhojpuri_labels, tokenizer, tag2id)\n",
        "\n",
        "# Load BERT model with a classification head\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_tags))\n",
        "\n",
        "# 🔄 Freeze all BERT parameters except classifier head\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 🔓 Unfreeze the last 4 layers for fine-tuning\n",
        "for layer in model.bert.encoder.layer[-6:]:  # Last 4 layers\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Training arguments with learning rate scheduling\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,  # Increased for better generalization\n",
        "    per_device_train_batch_size=8,  # Increased batch size\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.05,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,  # Adjusted for stability\n",
        "    report_to=[]  # Disable WandB logging\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate model on Bhojpuri test set\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).tolist()\n",
        "\n",
        "    for pred_sentence, true_sentence in zip(predictions, batch[\"labels\"].tolist()):\n",
        "        for pred_token, true_token in zip(pred_sentence, true_sentence):\n",
        "            if true_token != -100:\n",
        "                predicted_labels.append(id2tag[pred_token])\n",
        "                true_labels.append(id2tag[true_token])\n",
        "\n",
        "# Print final classification report\n",
        "print(\"\\n🔎 **Evaluation Results on Bhojpuri Test Set:**\")\n",
        "print(classification_report(true_labels, predicted_labels, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "jn89knbv75Gh",
        "outputId": "c2e1e8bb-98cb-4ea5-fcf1-ab91a82c3e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5/15 02:04 < 06:53, 0.02 it/s, Epoch 1.33/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.930538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 09:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.930538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.930072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.929657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.000200</td>\n",
              "      <td>2.929401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.000200</td>\n",
              "      <td>2.928984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 **Evaluation Results on Bhojpuri Test Set:**\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.03      0.09      0.05        47\n",
            "         ADP       0.00      0.00      0.00       147\n",
            "         ADV       0.00      0.00      0.00         3\n",
            "         AUX       0.06      0.12      0.08        34\n",
            "       CCONJ       0.03      0.29      0.06        21\n",
            "         DET       0.01      0.02      0.01        47\n",
            "        INTJ       0.00      0.00      0.00         0\n",
            "        NOUN       0.38      0.02      0.04       286\n",
            "         NUM       0.00      0.00      0.00        33\n",
            "        PART       0.00      0.00      0.00        16\n",
            "        PRON       0.00      0.00      0.00        37\n",
            "       PROPN       0.10      0.18      0.12       110\n",
            "       PUNCT       0.00      0.00      0.00       121\n",
            "       SCONJ       0.04      0.06      0.05        16\n",
            "        VERB       0.07      0.01      0.02        95\n",
            "           X       0.00      0.00      0.00         0\n",
            "           _       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.04      1013\n",
            "   macro avg       0.04      0.05      0.03      1013\n",
            "weighted avg       0.13      0.04      0.03      1013\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbwJhFg2FW8IngjrHR/Sdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}